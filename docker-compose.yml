version: "2.1"

services:
  hadoopmaster:
    build: 
      context: ./namenode
      dockerfile: Dockerfile
    container_name: hadoopmaster
    hostname: hadoopmaster
    ports:
      - 9870:9870
      - 8088:8088
      - 19888:19888
    environment:
      - CLUSTER_NAME=hadoop-cluster
    networks:
      - hadoop-cluster
    volumes:
      - namenode:/hadoop-data/dfs/name
  
  hadoopslave1:
    build: 
      context: ./datanode
      dockerfile: Dockerfile
    container_name: hadoopslave1
    hostname: hadoopslave1
    environment:
      - "HADOOP_MASTER=hadoopmaster"
    networks:
      - hadoop-cluster
    volumes:
      - datanode1:/hadoop-data/dfs/data

  hadoopslave2:
    build: 
      context: ./datanode
      dockerfile: Dockerfile
    container_name: hadoopslave2
    hostname: hadoopslave2
    environment:
      - "HADOOP_MASTER=hadoopmaster"
    networks:
      - hadoop-cluster
    volumes:
      - datanode2:/hadoop-data/dfs/data

  spark-client:
    build: 
      context: ./spark
      dockerfile: Dockerfile
    container_name: spark-client
    hostname: spark-client
    ports:
      - 4040:4040
      - 18080:18080
      - 9922:22
    volumes:
      - ./apps:/opt/spark-apps
      - ./data:/opt/spark-data
    networks:
      - hadoop-cluster

networks:
  hadoop-cluster:
    driver: bridge
  
volumes:
  namenode:
    driver: local
    name: namenodedata
  datanode1:
    driver: local
    name: datanode1data
  datanode2:
    driver: local
    name: datanode2data
  